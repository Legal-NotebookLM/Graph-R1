# QA Pair Format and Reasoning Explanation

## üìã QA Pair Format Required

### Raw JSON Files Format

The raw QA JSON files (e.g., `datasets/{data_source}/raw/qa_train.json`) should have the following structure:

```json
[
  {
    "question": "Your question text here",
    "golden_answers": ["answer1", "answer2", ...]  // Can be a list of strings
  },
  {
    "question": "Another question",
    "golden_answers": ["answer"]
  }
]
```

**Required Fields:**
- `question` (string): The question text
- `golden_answers` (list of strings): One or more correct answers

**Optional Fields (NOT required for training):**
- Any other fields in your JSON will be ignored by `script_process.py`

**Important Notes:**
- You do NOT need to include reasoning in the raw JSON files!
- You do NOT need to include context in the raw JSON files! Context is automatically collected during training/inference when the model queries the knowledge base.

---

## ü§î How Reasoning is Created (The Key Insight)

### Reasoning is NOT in the JSON - It's Generated by the Model!

The reasoning is **not provided** in your QA pair JSON files. Instead, it's **generated by the model** during training and inference. Here's how it works:

### 1. **Instruction Prompt Guides Reasoning Generation**

When processing QA pairs (in `script_process.py`), the system adds instructions that tell the model to generate reasoning:

```python
instruction_following = """Answer the given question. You can query from knowledge base provided to you to answer the question. You can query knowledge as many times as you want.
You must first conduct reasoning inside <think>...</think>. If you need to query knowledge, you can set a query statement between <query>...</query> to query from knowledge base after <think>...</think>.
When you have the final answer, you can output the answer inside <answer>...</answer>.

Output format for tool call:
<think>
...
</think>
<query>
...
</query>

Output format for answer:
<think>
...
</think>
<answer>
...
</answer>
"""
```

### 2. **The Model Generates Reasoning During Training**

During reinforcement learning (RL) training:

1. **Initial Generation**: The model receives the question with instructions
2. **Model Output**: The model generates text that includes:
   - `<think>...</think>` blocks (the reasoning)
   - `<query>...</query>` blocks (if knowledge is needed)
   - `<answer>...</answer>` blocks (the final answer)

3. **Reward Calculation**: The model's output (including reasoning) is evaluated:
   - Answer correctness (EM/F1 scores against `golden_answers`)
   - Reasoning quality (through reward models that evaluate logical coherence, factuality, etc.)

4. **Learning**: Through RL, the model learns to generate better reasoning because:
   - Good reasoning ‚Üí Better answers ‚Üí Higher rewards
   - The reward models evaluate reasoning quality (see `evaluation/eval_g.py`)

### 3. **Why This Works**

The model learns to generate reasoning because:

- **Instruction Following**: The prompt explicitly asks for reasoning
- **Reward Signal**: The reward models evaluate reasoning quality (logical coherence, factuality, etc.)
- **Self-Improvement**: Through RL, the model learns that good reasoning leads to better answers and higher rewards

---

## üìä Complete Data Flow

```
Raw JSON (qa_train.json)
  ‚Üì
  {
    "question": "...",
    "golden_answers": ["..."]
  }
  ‚Üì
script_process.py adds instructions
  ‚Üì
Processed Parquet
  {
    "prompt": [{
      "role": "user", 
      "content": "Answer the given question...\nYou must first conduct reasoning inside <think>...\nQuestion: ..."
    }],
    "reward_model": {
      "style": "rule",
      "ground_truth": ["..."]  // from golden_answers
    }
  }
  ‚Üì
Training (RL)
  ‚Üì
Model generates:
  <think>
  [Model creates reasoning here - NOT from JSON!]
  </think>
  <answer>
  [Final answer]
  </answer>
  ‚Üì
Reward models evaluate reasoning quality
  ‚Üì
Model learns to generate better reasoning
```

---

## üìå About Context Field

### Context is NOT in Your JSON - It's Collected During Training!

You might notice that evaluation code uses a `context` field, but **you don't need to provide it** in your raw JSON files.

**How Context Works:**
1. **During Training/Inference**: When the model queries the knowledge base using `<query>...</query>`, the retrieved knowledge chunks are automatically collected as "context"
2. **Context Collection**: The tool environment tracks all retrieved knowledge during the generation process
3. **Context Usage**: Context is used in evaluation to compute R-Sim (retrieval similarity) scores, which measure how well the model retrieved relevant information

**So:**
- ‚ùå **Don't include** `context` in your raw JSON files
- ‚úÖ Context is **automatically collected** during training when the model queries the knowledge base
- ‚úÖ Context is used for **evaluation metrics** (R-Sim score) to measure retrieval quality

---

## ‚úÖ Summary

1. **QA Pair Format**: Just `question` and `golden_answers` - no reasoning or context needed!

2. **Reasoning Generation**: 
   - The model generates reasoning during training/inference
   - Instructions in the prompt tell the model to include reasoning
   - Reward models evaluate reasoning quality
   - RL training teaches the model to generate better reasoning

3. **Context Collection**:
   - Context is automatically collected when the model queries the knowledge base
   - Used for evaluation (R-Sim scores) to measure retrieval quality
   - You don't need to provide it in your JSON files

4. **You don't need to provide reasoning or context** - both are generated/collected automatically:
   - Reasoning: Generated by the model through instruction following and RL
   - Context: Collected from knowledge base queries during training/inference

---

## üìù Example

**Your Raw JSON:**
```json
{
  "question": "What is the capital of France?",
  "golden_answers": ["Paris"]
}
```

**What the Model Generates (during training/inference):**
```
<think>
The question asks about the capital of France. I know that France is a country in Europe, and its capital city is Paris. This is a straightforward factual question.
</think>
<answer>
Paris
</answer>
```

The reasoning is **generated by the model**, not provided in your JSON!

